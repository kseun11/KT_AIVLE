{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"provenance":[{"file_id":"1ZX72JAO-IaAt5fPRKsUVof8JCHxgPaub","timestamp":1579183200965}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["## 데이터 출처\n","\n","[Naver sentiment movie corpus]: https://github.com/e9t/nsmc/\n","\n","- RNN 모델의 학습을 위해 [Naver sentiment movie corpus] 데이터셋 중 10,000건을 추출하여 사용하였습니다."],"metadata":{"id":"Fo92nKutwxk0"}},{"cell_type":"code","source":["# torchtext.legacy를 사용할 수 있는 torchtext 버전 설치\n","!pip install -U torchtext==0.10.0"],"metadata":{"id":"8WSzxoQ4Nd79","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665068893641,"user_tz":-540,"elapsed":103277,"user":{"displayName":"이정연","userId":"10444643468969229023"}},"outputId":"787d2a05-0f99-40ca-a703-af49c6521196"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchtext==0.10.0\n","  Downloading torchtext-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.1)\n","Collecting torch==1.9.0\n","  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n","\u001b[K     |████████████████████████████████| 831.4 MB 2.7 kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.1.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n","Installing collected packages: torch, torchtext\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.1+cu113\n","    Uninstalling torch-1.12.1+cu113:\n","      Successfully uninstalled torch-1.12.1+cu113\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.13.1\n","    Uninstalling torchtext-0.13.1:\n","      Successfully uninstalled torchtext-0.13.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\u001b[0m\n","Successfully installed torch-1.9.0 torchtext-0.10.0\n"]}]},{"cell_type":"code","metadata":{"id":"CSQVQtQVkMpg","outputId":"fcad5683-e919-444f-ee1a-10d7e6088952","executionInfo":{"status":"ok","timestamp":1665068914462,"user_tz":-540,"elapsed":20840,"user":{"displayName":"이정연","userId":"10444643468969229023"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["#colab 을 이용한 실행시\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"DDhMi_j7kI8u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665068914463,"user_tz":-540,"elapsed":15,"user":{"displayName":"이정연","userId":"10444643468969229023"}},"outputId":"51101c4b-e4fe-4ccd-93d4-91e70c513841"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# torchtext.legacy : text의 preprocessing 파이프라인 정의\n","# 1) 토크나이징(Tokenization)\n","# 2) 단어장 생성(Build Vocabulary)\n","# 3) 토큰의 수치화(Numericalize all tokens)\n","# 4) 데이터 로더 생성(Create Data Loader)\n","from torchtext.legacy import data\n","import torchtext.datasets as datasets\n","\n","import pickle\n","print (torch.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.9.0+cu102\n"]}]},{"cell_type":"markdown","source":["#LSTM\n","Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.\n","\n","nn.LSTM(input_size, hidden_size, num_layers, bidirectional, batch_first):\n","* input_size – The number of expected features in the input x\n","* hidden_size – The number of features in the hidden state h\n","* num_layers – Number of recurrent layers. \n","* bidirectional – If True, becomes a bidirectional LSTM. Default: False\n","* batch_first – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Default: False. Note that this does not apply to hidden or cell states."],"metadata":{"id":"_XfGe-AqK1NF"}},{"cell_type":"code","metadata":{"id":"MHVoD2RfkI81"},"source":["class RNN_Text(nn.Module):    \n","    def __init__(self, embed_num, class_num):\n","        # super()로 Base Class의 __init__() 호출 (nn.Module 클래스 생성자 호출)\n","        # super(파생클래스, self).__init__() 파이썬 2.x 문법\n","        # super().__init__() 파이썬 3.x 문법 둘다 사용 가능\n","        super(RNN_Text, self).__init__()\n","        \n","        V = embed_num   # 단어 사전의 크기\n","        C = class_num   # 분류하고자 하는 클래스 개수        \n","        H = 256         # 히든 사이즈\n","        D = 100         # 단어벡터 차원 100\n","        self.embed = nn.Embedding(V, D)\n","        \n","        # LSTM Layer, bidirectional이므로 출력되는 벡터의 크기는 H * 2\n","        self.rnn = nn.LSTM(D, H, bidirectional = True) # True : 양방향\n","\n","        # Linear Layer : (512, 2)\n","        self.out = nn.Linear(H*2, C)\n","        # H : 최종 output size (마지막 상태의 hidden state) * 2 (양방향)\n","        # C : 출력은 감성분석이기 때문에 2\n","        \n","    def forward(self, x):\n","        # Tokenize된 문장들을 embed\n","\n","        # 여기서 x가 해당 단어의 인덱스\n","        x = self.embed(x)     # (N, W, D) 문장 x의 단어 벡터값 가져옴. \n","        # (배치 사이즈 100, 입력 문장의 최대 길이 30, 단어 벡터 dimension)\n","\n","        # LSTM 모듈 실행\n","        # LSTM 입력데이터\n","        # input x : torch.Size([30, 100, 100]) [시퀀스 길이, 배치 사이즈, Dimension]\n","        # batch_first를 하지 않아서 1차원이 배치 사이즈\n","        # x = embedded x, (hidden, cell) 은 사용하지 않음\n","        # 한 문장이 들어갈 때 vector는 30 * 100\n","        x, (_, __) = self.rnn(x, (self.h, self.c))\n","\n","        # output x : torch.Size([30, 100, 512]) [시퀀스 길이, 배치 사이즈, 256 * 2]\n","        # 마지막 hidden state의 size가 512 (양방향이기 때문에 256 * 2)\n","        # print(\"output x size\". x.size())\n","\n","        # 최종 Hidden Layer로 Linear 모듈 실행\n","        # 분류 작업을 위한 Linear 모듈\n","        logit = self.out(x[-1])  # [30, 100, 512] 중 마지막 시점의 hidden state를 out (30의 마지막 sequence)\n","\n","        # 최종 예측 벡터 크기: [배치 사이즈, C], C: 클래스 개수\n","        return logit       # logit : torch.Size([100, 2]) # 2차원 vector가 100개 (batch=100)\n","\n","    # Cell, hidden state를 임의로 초기화\n","    def inithidden(self, b): # b는 batch_size\n","        # self.h = Variable(torch.randn(2, b, 256))\n","        # self.c = Variable(torch.randn(2, b, 256))    \n","        self.h = torch.randn(2, b, 256)   # [2, batch_size, 256]\n","        self.c = torch.randn(2, b, 256)   # [2, batch_size, 256]"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rnn = nn.LSTM(10, 20, 2) # input_size, hidden_size, num_layers\n","\n","input = torch.randn(5, 3, 10) # input (sequence length, batch size, input_size)\n","h0 = torch.randn(2, 3, 20) # hidden state (2 bidirectional, batch size, hidden size)\n","c0 = torch.randn(2, 3, 20) # cell state (2 bidirectional, batch size, hidden size)\n","\n","# output, (final hidden state, final cell state) = rnn(input, initial hidden state, initial cell state)\n","output, (hn, cn) = rnn(input,(h0, c0))"],"metadata":{"id":"vo62c8az4p8y"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fuVqEcvNkI83"},"source":["# train, test dataset을 만들어준다\n","class mydataset(data.Dataset):\n","    @staticmethod\n","    def sort_key(ex):\n","        return len(ex.text)\n","    def __init__(self, text_field, label_field, path=None, examples=None, **kwargs):\n","        fields = [('text', text_field), ('label', label_field)]\n","        if examples is None:\n","            path = self.dirname if path is None else path\n","            examples = []\n","            for i, line in enumerate(open(path,'r',encoding='utf-8')):\n","                if i==0:      # 첫번째 라인은 skip\n","                    continue\n","                line = line.strip().split('\\t') # text, label 필드가 /tab으로 구분되어 있다                  \n","                txt = line[1].split(' ')  # 공백을 기준으로 문자열을 나누어 토큰 리스트를 만든다. line[0]에는 ID\n","                \n","                # examples: 학습 텍스트, 라벨 텍스트\n","                # data.Example : Defines a single training or test example.\n","                examples += [ data.Example.fromlist( [txt, line[2]],fields ) ]\n","        # Create a dataset from a list of Examples and Fields.\n","        # fields : field name, field\n","        super(mydataset, self).__init__(examples, fields, **kwargs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D-YgsUg5kI86","outputId":"81fabc9e-c986-41cc-8432-cb156a8e0752","executionInfo":{"status":"ok","timestamp":1665032235190,"user_tz":-540,"elapsed":2344,"user":{"displayName":"이정연","userId":"10444643468969229023"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["# Field 객체는 다음과 같은 값을 통하여 데이터의 각 필드를 처리하는 방법을 지정\n","# fix_length: A fixed length that all examples using this field will be padded to, or None for flexible sequence lengths. \n","# sequential: Whether the datatype represents sequential data. If False, no tokenization is applied. Default: True.\n","# batch_first: Whether to produce tensors with the batch dimension first. Default: False.\n","##text_field = data.Field(fix_length=20)\n","text_field = data.Field(fix_length=30) # 아래에서 30\n","label_field = data.Field(sequential=False, batch_first = True, unk_token = None)\n","\n","# 학습데이터 Dataset\n","train_data = mydataset(text_field,label_field,path='/content/gdrive/My Drive/Colab Notebooks/KT_RNN/data/nsm/small_ratings_train_tok.txt')\n","# 테스트데이터 Dataset\n","test_data = mydataset(text_field,label_field,path='/content/gdrive/My Drive/Colab Notebooks/KT_RNN/data/nsm/small_ratings_test_tok.txt')\n","\n","text_field.build_vocab(train_data)    # Construct the Vocab object\n","label_field.build_vocab(train_data)   # Construct the Vocab object\n","\n","# Create Iterator objects for train data, test data (data loader)\n","train_iter, test_iter = data.Iterator.splits(\n","                            (train_data, test_data),\n","                            batch_sizes=(100, 1), repeat=False)#, device = -1) # 100개 묶음으로 설정\n","len(text_field.vocab) # 21893개의 단어 (10000개 정도의 문장을 tokenize한 결과)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["21893"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"WPOJtjBWkI88","outputId":"8bad46db-3e48-486b-a143-e49ff1b20c1a","executionInfo":{"status":"ok","timestamp":1665032238506,"user_tz":-540,"elapsed":389,"user":{"displayName":"이정연","userId":"10444643468969229023"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["rnn = RNN_Text(len(text_field.vocab),2)     # embed_num, class_num\n","optimizer = torch.optim.Adam(rnn.parameters())\n","rnn.train() # train mode로"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RNN_Text(\n","  (embed): Embedding(21893, 100)\n","  (rnn): LSTM(100, 256, bidirectional=True)\n","  (out): Linear(in_features=512, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"HEGfWoiykI8_","outputId":"1c2ed623-789c-4d6c-9be2-d24e2701dfb8","executionInfo":{"status":"ok","timestamp":1665033715518,"user_tz":-540,"elapsed":196637,"user":{"displayName":"이정연","userId":"10444643468969229023"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["%%time\n","bool_debug = True    # 텐서의 차원을 출력할 경우 True로 설정\n","print_idx = 3        # 출력 횟수\n","for epoch in range(10):\n","    \n","    totalloss = 0\n","    for batch in train_iter: # batch 단위로 전달\n","        optimizer.zero_grad()\n","        \n","        # batch는 text와 label field로 구성됨.\n","\n","        # 문장 부분\n","        txt = batch.text        # torch.Size([30, 100])\n","        # 1, 0 레이블값\n","        label = batch.label     # torch.Size([100])\n","        \n","        # debug를 이용한 출력\n","        if bool_debug and print_idx > 0:\n","          print (\"txt.shape:\", txt.shape)\n","          print_idx -= 1 # 출력을 3번만 해보자\n","\n","        # inithiddend : hidden state, cell state 초기화 함수\n","        rnn.inithidden(txt.size(1))   # 배치 사이즈를 전달 (torch.Size의 2번째 차원인 100)\n","        # 학습 실행 (forward 함수가 자동으로 호출)\n","        pred = rnn(txt) # batch 단위로 돌기 때문의 output의 dimension은 (100, 2)\n","        \n","        # debug를 이용한 출력\n","        if bool_debug and print_idx > 0:\n","          print(\"pred.shape:\", pred.shape)\n","          print(\"label.shape:\", label.shape)\n","          print_idx -= 1        \n","        \n","        loss = F.cross_entropy(pred, label)\n","        totalloss += loss.data\n","        \n","        loss.backward()\n","        optimizer.step()\n","        \n","    print(epoch,'epoch')  \n","    print('loss : {:.3f}'.format(totalloss.numpy()))\n","       \n","torch.save(rnn,'/content/gdrive/My Drive/Colab Notebooks/KT_RNN/model/rnn_model.pt')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["txt.shape: torch.Size([30, 100])\n","pred.shape: torch.Size([100, 2])\n","label.shape: torch.Size([100])\n","txt.shape: torch.Size([30, 100])\n","0 epoch\n","loss : 69.896\n","1 epoch\n","loss : 69.169\n","2 epoch\n","loss : 66.247\n","3 epoch\n","loss : 54.412\n","4 epoch\n","loss : 42.760\n","5 epoch\n","loss : 33.788\n","6 epoch\n","loss : 26.233\n","7 epoch\n","loss : 19.618\n","8 epoch\n","loss : 14.459\n","9 epoch\n","loss : 11.121\n","CPU times: user 6min 26s, sys: 6.87 s, total: 6min 32s\n","Wall time: 3min 16s\n"]}]},{"cell_type":"code","metadata":{"id":"ldrqhEt6kI9B","outputId":"1f89eba0-d039-4757-fe8a-ee7dbd10c952","executionInfo":{"status":"ok","timestamp":1665034766225,"user_tz":-540,"elapsed":1394,"user":{"displayName":"이정연","userId":"10444643468969229023"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["%%time\n","bool_debug = True    # 텐서의 차원을 출력할 경우 True로 설정\n","\n","from sklearn.metrics import classification_report\n","correct = 0\n","incorrect = 0\n","rnn.eval()\n","y_test = [] # labels를 미리 추가\n","prediction = []\n","\n","# 텐서 차원 확인용\n","print_tensor_shape = 2\n","print_idx = 1\n","\n","for batch in test_iter: # test dataloader\n","    txt = batch.text            # txt.shape: torch.Size([max_sent_len, 1]) # 30, 1\n","    label = batch.label         # label.shape: torch.Size([1])\n","    y_test.append(label.data[0])\n","    \n","    rnn.inithidden(txt.size(1)) # batch size를 넘겨줌 (1)\n","   \n","    pred = rnn(txt)               # pred.shape: torch.Size([1, 2])\n","    \n","    # 큰값, 작은값 -> ans = 1\n","    _ , ans = torch.max(pred,dim=1) # ans.shape: torch.Size([1])\n","    prediction.append(ans.data[0])\n","    \n","    \n","    #---------------------------------------\n","    # 텐서 형태, 데이터를 출력\n","    if bool_debug and print_tensor_shape > 0:\n","      print(\"-----\", print_idx, \"-----\") \n","      print(\"prediction:\", prediction)\n","      print(\"y_test:\", y_test)\n","      print(\"pred.shape:\", pred.shape)\n","      #print(\"pred.data[0]:\", pred.data[0])\n","      print(\"pred[0]:\", pred[0])\n","      print(\"pred[0][0]:\", pred[0][0])\n","      print(\"pred[0][1]:\", pred[0][1])\n","      print(\"ans.data[0]:\", ans.data[0])\n","      print(\"ans.shape:\", ans.shape)\n","      print(\"txt.shape:\", txt.shape)\n","      print(\"label.shape:\", label.shape)\n","      print(\"label.data[0]:\", label.data[0])\n","      \n","      print()\n","      print_tensor_shape -= 1\n","      print_idx += 1\n","      #---------------------------------------\n","\n","    # 예측값 = label값\n","    if ans.data[0] == label.data[0]:  # ans.data[0]: tensor(0) 또는 tensor(1)\n","        correct += 1    \n","    else:\n","        incorrect += 1\n","    \n","print ('correct : ', correct)\n","print ('incorrect : ', incorrect)\n","print(classification_report(torch.tensor(y_test), \n","                            torch.tensor(prediction), \n","                            digits=4, \n","                            target_names=['negative', 'positive']))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----- 1 -----\n","prediction: [tensor(0)]\n","y_test: [tensor(1)]\n","pred.shape: torch.Size([1, 2])\n","pred[0]: tensor([ 1.0333, -1.0223], grad_fn=<SelectBackward>)\n","pred[0][0]: tensor(1.0333, grad_fn=<SelectBackward>)\n","pred[0][1]: tensor(-1.0223, grad_fn=<SelectBackward>)\n","ans.data[0]: tensor(0)\n","ans.shape: torch.Size([1])\n","txt.shape: torch.Size([30, 1])\n","label.shape: torch.Size([1])\n","label.data[0]: tensor(1)\n","\n","----- 2 -----\n","prediction: [tensor(0), tensor(0)]\n","y_test: [tensor(1), tensor(1)]\n","pred.shape: torch.Size([1, 2])\n","pred[0]: tensor([ 0.5639, -0.5466], grad_fn=<SelectBackward>)\n","pred[0][0]: tensor(0.5639, grad_fn=<SelectBackward>)\n","pred[0][1]: tensor(-0.5466, grad_fn=<SelectBackward>)\n","ans.data[0]: tensor(0)\n","ans.shape: torch.Size([1])\n","txt.shape: torch.Size([30, 1])\n","label.shape: torch.Size([1])\n","label.data[0]: tensor(1)\n","\n","correct :  83\n","incorrect :  17\n","              precision    recall  f1-score   support\n","\n","    negative     0.7619    0.9600    0.8496        50\n","    positive     0.9459    0.7000    0.8046        50\n","\n","    accuracy                         0.8300       100\n","   macro avg     0.8539    0.8300    0.8271       100\n","weighted avg     0.8539    0.8300    0.8271       100\n","\n","CPU times: user 1.15 s, sys: 36.3 ms, total: 1.18 s\n","Wall time: 861 ms\n"]}]},{"cell_type":"code","metadata":{"id":"yEWu-5QokI9E"},"source":[],"execution_count":null,"outputs":[]}]}